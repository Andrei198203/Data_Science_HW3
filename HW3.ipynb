{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIr5/w5gR4I6t5G+jKZhSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrei198203/Data_Science_HW3/blob/main/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Y9iNpBi8XIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task1"
      ],
      "metadata": {
        "id": "TS3Bi6lu8ZFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QP0cNvaY7JM7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def hypothesis(X, theta):\n",
        "    \"\"\"\n",
        "    Calculates a linear regression hypothesis in vector form.\n",
        "\n",
        "    Parameters:\n",
        "    X -- feature matrix (rows are examples, columns are features)\n",
        "    theta -- a vector of parameters (including the shift)\n",
        "\n",
        "    Returns:\n",
        "    h -- vector of hypotheses (predicted values)\n",
        "    \"\"\"\n",
        "    h = np.dot(X, theta)\n",
        "    return h"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task2"
      ],
      "metadata": {
        "id": "uwqL1Uel8dKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, theta):\n",
        "    \"\"\"\n",
        "    Calculates the loss function for a linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    X -- feature matrix (rows are examples, columns are features)\n",
        "    y - vector of target values\n",
        "    theta -- a vector of parameters (including bias)\n",
        "\n",
        "    Returns:\n",
        "    J -- the value of the loss function\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    h = hypothesis(X, theta)\n",
        "    J = (1/(2*m)) * np.sum(np.square(h - y))\n",
        "    return J"
      ],
      "metadata": {
        "id": "eCAnhkas7vQh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task3"
      ],
      "metadata": {
        "id": "SpI1Yopn8fsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_step(X, y, theta, alpha):\n",
        "    \"\"\"\n",
        "    Performs a single gradient descent step for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    X -- feature matrix (rows are examples, columns are features)\n",
        "    y - vector of target values\n",
        "    theta - vector of parameters (including the shift)\n",
        "    alpha -- gradient descent step\n",
        "\n",
        "    Returns:\n",
        "    theta -- updated parameter values after one step\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    h = hypothesis(X, theta)\n",
        "    gradient = (1/m) * np.dot(X.T, (h - y))\n",
        "    theta -= alpha * gradient\n",
        "    return theta"
      ],
      "metadata": {
        "id": "JBpUIOZZ73iZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task4"
      ],
      "metadata": {
        "id": "HcidRj8F8ij5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# Linear regression hypothesis function\n",
        "def hypothesis(X, theta):\n",
        "    return np.dot(X, theta)\n",
        "\n",
        "# Function for calculating the loss function\n",
        "def compute_cost(X, y, theta):\n",
        "    m = len(y)\n",
        "    h = hypothesis(X, theta)\n",
        "    J = (1/(2*m)) * np.sum(np.square(h - y))\n",
        "    return J\n",
        "\n",
        "# One step of gradient descent\n",
        "def gradient_descent_step(X, y, theta, alpha):\n",
        "    m = len(y)\n",
        "    h = hypothesis(X, theta)\n",
        "    gradient = (1/m) * np.dot(X.T, (h - y))\n",
        "    theta -= alpha * gradient\n",
        "    return theta\n",
        "\n",
        "# Data normalisation\n",
        "def normalize_features(X):\n",
        "    \"\"\"\n",
        "    Normalises the features to the range [0, 1].\n",
        "\n",
        "    Parameters:\n",
        "    X -- feature matrix (rows are examples, columns are features)\n",
        "\n",
        "    Returns:\n",
        "    X_norm -- normalised features\n",
        "    \"\"\"\n",
        "    mins = np.min(X, axis=0)\n",
        "    maxs = np.max(X, axis=0)\n",
        "    X_norm = (X - mins) / (maxs - mins)\n",
        "    return X_norm\n",
        "\n",
        "# Link to file\n",
        "url = \"https://drive.google.com/uc?id=1-rAa4XT4_fI0dOBlMNuE6a7jB0wln_Qo\"\n",
        "\n",
        "# Downloading data\n",
        "response = requests.get(url)\n",
        "content = response.content.decode('utf-8')\n",
        "data = pd.read_csv(StringIO(content))\n",
        "\n",
        "# Separation into attributes and target variable\n",
        "X = data[['area', 'bathrooms', 'bedrooms']].values\n",
        "y = data['price'].values\n",
        "\n",
        "# Normalisation of features\n",
        "X_normalized = normalize_features(X)\n",
        "\n",
        "# Add a column with units (offset)\n",
        "X_normalized = np.c_[X_normalized, np.ones(X_normalized.shape[0])]\n",
        "\n",
        "# Initialising parameters\n",
        "theta = np.zeros(X_normalized.shape[1])\n",
        "\n",
        "# Gradient descent parameters\n",
        "alpha = 0.01\n",
        "iterations = 1500\n",
        "\n",
        "# Gradient descent\n",
        "for _ in range(iterations):\n",
        "    theta = gradient_descent_step(X_normalized, y, theta, alpha)\n",
        "\n",
        "print(\"The best parameters found using gradient descent:\")\n",
        "print(theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcHBkP9J8j4w",
        "outputId": "e48fc883-3c8c-4181-8fa7-57fe71eaed9e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best parameters found using gradient descent:\n",
            "[2312999.60105339 1886635.22246029 1995918.88602716 3279504.28376392]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task5"
      ],
      "metadata": {
        "id": "0Uh-YrGwBX7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta_analytical = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "print(\"The best parameters found with the help of an analytical solution:\")\n",
        "print(theta_analytical)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27rHUiO_BZ_B",
        "outputId": "ce9a2062-cde8-42d5-f7e5-0331dbdea557"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best parameters found with the help of an analytical solution:\n",
            "[3.72448352e+02 1.37031315e+06 3.68974672e+05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task6"
      ],
      "metadata": {
        "id": "f-tImHtlBrsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Comparison of results:\")\n",
        "print(\"Gradient descent:\", theta)\n",
        "print(\"Analytical solution:\", theta_analytical)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRO5kEZLBtvi",
        "outputId": "904252e9-fe41-4ed0-9089-343f3df668f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of results:\n",
            "Gradient descent: [2312999.60105339 1886635.22246029 1995918.88602716 3279504.28376392]\n",
            "Analytical solution: [3.72448352e+02 1.37031315e+06 3.68974672e+05]\n"
          ]
        }
      ]
    }
  ]
}